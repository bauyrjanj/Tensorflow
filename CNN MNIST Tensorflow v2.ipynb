{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of:\n",
      " - Training-set:\t\t60000\n",
      " - Test-set:\t\t\t10000\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of:\")\n",
    "print(\" - Training-set:\\t\\t{}\".format(len(x_train)))\n",
    "print(\" - Test-set:\\t\\t\\t{}\".format(len(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (60000,), (10000, 28, 28), (10000,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Memory size of x_train = 47.040128 MBs\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(\"The Memory size of x_train = {} MBs\".format(int(sys.getsizeof(x_train))/1000000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper function for plotting the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(images, cls_true, cls_pred=None):\n",
    "    assert len(images) == len(cls_true) == 9\n",
    "    \n",
    "    # Create figure with 3x3 sub-plots.\n",
    "    fig, axes = plt.subplots(3, 3)\n",
    "    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        # Plot image.\n",
    "        ax.imshow(images[i].reshape(img_shape), cmap='binary')\n",
    "\n",
    "        # Show true and predicted classes.\n",
    "        if cls_pred is None:\n",
    "            xlabel = \"True: {0}\".format(cls_true[i])\n",
    "        else:\n",
    "            xlabel = \"True: {0}, Pred: {1}\".format(cls_true[i], cls_pred[i])\n",
    "\n",
    "        # Show the classes as the label on the x-axis.\n",
    "        ax.set_xlabel(xlabel)\n",
    "        \n",
    "        # Remove ticks from the plot.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    \n",
    "    # Ensure the plot is shown correctly with multiple plots\n",
    "    # in a single Notebook cell.\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We know that MNIST images are 28 pixels by 28 pixels\n",
    "img_size = 28\n",
    "\n",
    "# Images are stored in one-dimensional arrays of this length - number of numbers in the matrix of size 28 x 28\n",
    "img_size_flat = img_size*img_size\n",
    "\n",
    "# Height and width of each image in Tuple\n",
    "img_shape = (img_size, img_size)\n",
    "\n",
    "# Number of color channels for the image: 1 channel for gray-scale\n",
    "num_channels = 1\n",
    "\n",
    "# Number of classes, one class for each of 10 digits\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualise Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the first images from the train set.\n",
    "images = x_train[0:9]\n",
    "\n",
    "# Get the true classes for those images.\n",
    "cls_true = y_train[0:9]\n",
    "\n",
    "# Plot the images and labels using our helper-function above.\n",
    "plot_images(images=images, cls_true=cls_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters of the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional layer 1\n",
    "filter1_size = 5  #Convolution filters are 5 x 5 pixels\n",
    "num_filters1 = 16 #There are 16 of these filters\n",
    "\n",
    "# Convolutional layer 2\n",
    "filter2_size = 5 #Convolution filters are 5 x 5 pixels\n",
    "num_filters2 = 32 #There are 32 of these filters\n",
    "\n",
    "# Pooling\n",
    "window_size = 2 #Pooling window 2x2\n",
    "window_stride = 2 #Move by 2 strides\n",
    "\n",
    "# Fully-connected layer\n",
    "fc_size=128     # Number of nodes in the fully-connected layer\n",
    "\n",
    "# Convolution stride\n",
    "conv_stride=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper function for Creating Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights(shape):\n",
    "    weights = tf.Variable(tf.random.normal(shape=shape, stddev=0.05))\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvNet 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvNet1(image):\n",
    "    # Conv1 layer\n",
    "    shape_1 = [filter1_size, filter1_size, num_channels, num_filters1]\n",
    "    shape_bias1 = [num_channels, img_size, img_size, num_filters1]\n",
    "    conv1_weights = weights(shape_1)\n",
    "    bias_1 = tf.Variable(tf.ones(shape=shape_bias1))\n",
    "    conv1 = tf.nn.conv2d(input=image, filters=conv1_weights, strides=conv_stride, padding='SAME')\n",
    "    conv1+=bias_1\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "    conv1 = tf.nn.max_pool(input=conv1, ksize=window_size, strides=window_stride, padding='SAME')\n",
    "    return conv1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvNet2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvNet2(conv1):\n",
    "    # Conv2 Layer\n",
    "    shape_2 = [filter2_size, filter2_size, num_filters1, num_filters2]\n",
    "    shape_bias2 = [num_channels, conv1.shape[1], conv1.shape[1], num_filters2]\n",
    "    conv2_weights = weights(shape_2)\n",
    "    bias_2 = tf.Variable(tf.ones(shape=shape_bias2))\n",
    "    conv2 = tf.nn.conv2d(input=conv1, filters=conv2_weights, strides=conv_stride, padding='SAME')\n",
    "    conv2+=bias_2\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "    conv2 = tf.nn.max_pool(input=conv2, ksize=window_size, strides=window_stride, padding='SAME')\n",
    "    return conv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flatten Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_layer(conv2):\n",
    "    layer_shape = conv2.get_shape()\n",
    "    num_features = layer_shape[1:4].num_elements()\n",
    "    flat_layer = tf.reshape(conv2, [-1, num_features])\n",
    "    return flat_layer, num_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Connected Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully connected layer\n",
    "num_features=7*7*32 # 7 x 7 filtered images of 32 pieces for each image\n",
    "fc_shape = [num_features, fc_size]\n",
    "fc_weights = tf.Variable(tf.random.normal(shape=fc_shape))\n",
    "bias_fc = tf.Variable(tf.ones([fc_size]))\n",
    "# Output layer\n",
    "shape_out = [fc_size, num_classes]\n",
    "w_out = tf.Variable(tf.random.normal(shape=shape_out))\n",
    "b_out = tf.Variable(tf.ones([num_classes]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(flat_layer, fc_weights, bias_fc, w_out, b_out):\n",
    "    #Fully connected layer\n",
    "    fc_product = tf.matmul(flat_layer, fc_weights)\n",
    "    fully_connected = tf.keras.activations.relu(fc_product+bias_fc)\n",
    "    # Output layer\n",
    "    output = tf.matmul(fully_connected, w_out)\n",
    "    prediction = tf.keras.activations.softmax(output + b_out)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(image):\n",
    "    conv1 = ConvNet1(image)\n",
    "    conv2 = ConvNet2(conv1)\n",
    "    flat_layer, num_features = flatten_layer(conv2)\n",
    "    predictions = make_prediction(flat_layer, fc_weights, bias_fc, w_out, b_out)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile and Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a loss function.\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the metrics.\n",
    "train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "val_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the training data set\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Prepare the input data \n",
    "x_training = tf.constant(x_train, tf.float32)\n",
    "y_training = tf.constant(y_train, tf.int64)\n",
    "# Prepare the input data\n",
    "x_testing = tf.constant(x_test, tf.float32)\n",
    "y_testing = tf.constant(y_test, tf.int64)\n",
    "\n",
    "# Reshape as required for ConvNet\n",
    "new_shape = [-1, img_size, img_size, num_channels]\n",
    "x_training = tf.reshape(tensor = x_training, shape = new_shape)\n",
    "#y_training = tf.one_hot(y_training,depth=10) # One-hot encoding of the target variable as required by the model\n",
    "x_testing = tf.reshape(tensor = x_testing, shape = new_shape)\n",
    "#y_testing = tf.one_hot(y_testing,depth=10) # One-hot encoding of the target variable as required by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([60000, 28, 28, 1]),\n",
       " TensorShape([10000, 28, 28, 1]),\n",
       " TensorShape([60000]),\n",
       " TensorShape([10000]))"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_training.shape, x_testing.shape, y_training.shape, y_testing.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package the data into batches\n",
    "batch_size=32\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_training, y_training)).shuffle(1024).batch(batch_size)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_testing, y_testing)).batch(batch_size)\n",
    "\n",
    "# Prepare the validation dataset.\n",
    "# Reserve 10,000 samples for validation.\n",
    "x_val = x_training[-10000:]\n",
    "y_val = y_training[-10000:]\n",
    "x_train = x_training[:-10000]\n",
    "y_train = y_training[:-10000]\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_dataset = val_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, x_batch_train size: (32, 28, 28, 1), y_batch_train size: (32,)\n"
     ]
    }
   ],
   "source": [
    "for step, (x_batch_train, y_batch_train) in enumerate(train_ds):\n",
    "    print(\"step: {}, x_batch_train size: {}, y_batch_train size: {}\".format(step, x_batch_train.shape, y_batch_train.shape))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 1874: 2.3362\n",
      "Seen so far: 60000 samples\n",
      "Training acc over epoch: 0.1250\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 1874: 2.3362\n",
      "Seen so far: 60000 samples\n",
      "Training acc over epoch: 0.1250\n",
      "Time taken: 84.08s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "epochs = 2\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "    \n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_ds):\n",
    "\n",
    "        # Open a GradientTape to record the operations run\n",
    "        # during the forward pass, which enables auto-differentiation.\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            # Run the forward pass of the layer.\n",
    "            # The operations that the layer applies\n",
    "            # to its inputs are going to be recorded\n",
    "            # on the GradientTape.\n",
    "            logits = model(x_batch_train)  # Logits for this minibatch\n",
    "\n",
    "            # Compute the loss value for this minibatch.\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "\n",
    "        # Use the gradient tape to automatically retrieve\n",
    "        # the gradients of the trainable variables with respect to the loss.\n",
    "        grads = tape.gradient(loss_value, [fc_weights, bias_fc, w_out, b_out])\n",
    "\n",
    "        # Run one step of gradient descent by updating\n",
    "        # the value of the variables to minimize the loss.\n",
    "        optimizer.apply_gradients(zip(grads, [fc_weights, bias_fc, w_out, b_out]))\n",
    "        \n",
    "        # Update training metric.\n",
    "        train_acc_metric.update_state(y_batch_train, logits)\n",
    "\n",
    "        # Log every 1874 batches.\n",
    "        if step%1874 == 0 and step!=0:\n",
    "            print(\n",
    "                \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                % (step, float(loss_value))\n",
    "            )\n",
    "            print(\"Seen so far: %s samples\" % ((step + 1) * 32))\n",
    "            \n",
    "         # Display metrics at the end of every 1874 batch\n",
    "        if step%1874==0 and step!=0:\n",
    "            train_acc = train_acc_metric.result()\n",
    "            print(\"Training acc: %.4f\" % (float(train_acc),))\n",
    "\n",
    "        # Reset training metrics at the end of each epoch\n",
    "        train_acc_metric.reset_states()\n",
    "\n",
    "        # Run a validation loop at the end of each epoch.\n",
    "        #for x_batch_val, y_batch_val in val_dataset:\n",
    "        #    val_logits = model(x_batch_val)\n",
    "            # Update val metrics\n",
    "        #    val_acc_metric.update_state(y_batch_val, val_logits)\n",
    "        #val_acc = val_acc_metric.result()\n",
    "        #val_acc_metric.reset_states()\n",
    "        #print(\"Validation acc: %.4f\" % (float(val_acc),))\n",
    "print(\"Time taken: %.2fs\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
