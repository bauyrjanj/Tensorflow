{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of:\n",
      " - Training-set:\t\t60000\n",
      " - Test-set:\t\t\t10000\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of:\")\n",
    "print(\" - Training-set:\\t\\t{}\".format(len(x_train)))\n",
    "print(\" - Test-set:\\t\\t\\t{}\".format(len(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (60000,), (10000, 28, 28), (10000,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Memory size of x_train = 47.040128 MBs\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(\"The Memory size of x_train = {} MBs\".format(int(sys.getsizeof(x_train))/1000000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper function for plotting the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(images, cls_true, cls_pred=None):\n",
    "    assert len(images) == len(cls_true) == 9\n",
    "    \n",
    "    # Create figure with 3x3 sub-plots.\n",
    "    fig, axes = plt.subplots(3, 3)\n",
    "    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        # Plot image.\n",
    "        ax.imshow(images[i].reshape(img_shape), cmap='binary')\n",
    "\n",
    "        # Show true and predicted classes.\n",
    "        if cls_pred is None:\n",
    "            xlabel = \"True: {0}\".format(cls_true[i])\n",
    "        else:\n",
    "            xlabel = \"True: {0}, Pred: {1}\".format(cls_true[i], cls_pred[i])\n",
    "\n",
    "        # Show the classes as the label on the x-axis.\n",
    "        ax.set_xlabel(xlabel)\n",
    "        \n",
    "        # Remove ticks from the plot.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    \n",
    "    # Ensure the plot is shown correctly with multiple plots\n",
    "    # in a single Notebook cell.\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We know that MNIST images are 28 pixels by 28 pixels\n",
    "img_size = 28\n",
    "\n",
    "# Images are stored in one-dimensional arrays of this length - number of numbers in the matrix of size 28 x 28\n",
    "img_size_flat = img_size*img_size\n",
    "\n",
    "# Height and width of each image in Tuple\n",
    "img_shape = (img_size, img_size)\n",
    "\n",
    "# Number of color channels for the image: 1 channel for gray-scale\n",
    "num_channels = 1\n",
    "\n",
    "# Number of classes, one class for each of 10 digits\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualise Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the first images from the train set.\n",
    "images = x_train[0:9]\n",
    "\n",
    "# Get the true classes for those images.\n",
    "cls_true = y_train[0:9]\n",
    "\n",
    "# Plot the images and labels using our helper-function above.\n",
    "plot_images(images=images, cls_true=cls_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters of the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional layer 1\n",
    "filter1_size = 5  #Convolution filters are 5 x 5 pixels\n",
    "num_filters1 = 16 #There are 16 of these filters\n",
    "\n",
    "# Convolutional layer 2\n",
    "filter2_size = 5 #Convolution filters are 5 x 5 pixels\n",
    "num_filters2 = 32 #There are 32 of these filters\n",
    "\n",
    "# Pooling\n",
    "window_size = 2 #Pooling window 2x2\n",
    "window_stride = 2 #Move by 2 strides\n",
    "\n",
    "# Fully-connected layer\n",
    "fc_size=128     # Number of nodes in the fully-connected layer\n",
    "\n",
    "# Convolution stride\n",
    "conv_stride=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform the input data into the required shape/format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the input data - Just taking the first 1000 images stay within the compute resource limitation\n",
    "x_training = tf.constant(x_train[:1000], tf.float32)\n",
    "y_training = tf.constant(y_train[:1000], tf.int64)\n",
    "# Prepare the input data\n",
    "x_testing = tf.constant(x_test[:1000], tf.float32)\n",
    "y_testing = tf.constant(y_test[:1000], tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([1000, 28, 28]), TensorShape([1000]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_training.shape, y_training.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=int64, numpy=array([5, 0, 4, 1, 9])>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_training[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape as required for ConvNet\n",
    "new_shape = [-1, img_size, img_size, num_channels]\n",
    "x_image = tf.reshape(tensor = x_training, shape = new_shape)\n",
    "y_image = tf.one_hot(y_training,depth=10) # One-hot encoding of the target variable as required by the model\n",
    "x = tf.reshape(tensor = x_testing, shape = new_shape)\n",
    "y = tf.one_hot(y_testing,depth=10) # One-hot encoding of the target variable as required by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1000, 28, 28, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1000, 10])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package the data into batches\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_image, y_image)).shuffle(10000).batch(32)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x, y)).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((None, 28, 28, 1), (None, 10)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((None, 28, 28, 1), (None, 10)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper function for Creating Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights(shape):\n",
    "    weights = tf.Variable(tf.random.normal(shape=shape, stddev=0.05))\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper function for Flat Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_layer(layer):\n",
    "    # Get the shape of the layer\n",
    "    layer_shape = layer.get_shape()\n",
    "    # Calculate the number of features for each image which is img_height * img_width * num_channels\n",
    "    num_features = layer_shape[1:4].num_elements()\n",
    "    # Reshape the layer to [num_images, num_features].\n",
    "    # Note that we just set the size of the second dimension\n",
    "    # to num_features and the size of the first dimension to -1\n",
    "    # which means the size in that dimension is calculated\n",
    "    # so the total size of the tensor is unchanged from the reshaping.\n",
    "    flat_layer = tf.reshape(layer, [-1, num_features])\n",
    "    return flat_layer, num_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Weights of the Fully Connected Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the architecture of the model\n",
    "def nn_forward(image):\n",
    "    # Conv1 layer\n",
    "    shape_1 = [5, 5, 1, 16]\n",
    "    shape_bias1 = [1, 28, 28, 16]\n",
    "    conv1_weights = weights(shape_1)\n",
    "    bias_1 = tf.Variable(tf.ones(shape=shape_bias1))\n",
    "    conv1 = tf.nn.conv2d(input=image, filters=conv1_weights, strides=1, padding='SAME')\n",
    "    conv1+=bias_1\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "    conv1 = tf.nn.max_pool(input=conv1, ksize=2, strides=2, padding='SAME')\n",
    "    # Conv2 Layer\n",
    "    shape_2 = [5, 5, 16, 32]\n",
    "    shape_bias2 = [1, conv1.shape[1], conv1.shape[1], 32]\n",
    "    conv2_weights = weights(shape_2)\n",
    "    bias_2 = tf.Variable(tf.ones(shape=shape_bias2))\n",
    "    conv2 = tf.nn.conv2d(input=conv1, filters=conv2_weights, strides=1, padding='SAME')\n",
    "    conv2+=bias_2\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "    conv2 = tf.nn.max_pool(input=conv2, ksize=2, strides=2, padding='SAME')\n",
    "    # Flat Layer\n",
    "    flat_layer, num_features = flatten_layer(conv2)\n",
    "    return flat_layer, num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully connected layer\n",
    "fc_shape = [1568, fc_size]\n",
    "fc_weights = tf.Variable(tf.random.normal(shape=fc_shape))\n",
    "bias_fc = tf.Variable(tf.ones([fc_size]))\n",
    "# Output layer\n",
    "shape_out = [fc_size, num_classes]\n",
    "w_out = tf.Variable(tf.random.normal(shape=shape_out))\n",
    "b_out = tf.Variable(tf.ones([num_classes]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper-function for the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_loss_function(image, actual):\n",
    "    flat_layer, num_features = nn_forward(image)\n",
    "    #Fully connected layer\n",
    "    fc_product = tf.matmul(flat_layer, fc_weights)\n",
    "    fully_connected = tf.keras.activations.relu(fc_product+bias_fc)\n",
    "    # Output layer\n",
    "    output = tf.matmul(fully_connected, w_out)\n",
    "    prediction = tf.keras.activations.softmax(output + b_out)\n",
    "    loss = tf.keras.losses.categorical_crossentropy(actual, prediction)\n",
    "    return loss, prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile and Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.metrics.Mean at 0x7f51b8325da0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss, predictions = nn_loss_function(images, labels)\n",
    "    gradients = tape.gradient(loss, [fc_weights, bias_fc, w_out, b_out])\n",
    "    optimizer.apply_gradients(zip(gradients, [fc_weights, bias_fc, w_out, b_out]))\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "    t_loss, predictions = nn_loss_function(labels)\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((None, 28, 28, 1), (None, 10)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    <ipython-input-47-33adeeb473f7>:8 train_step  *\n        train_accuracy(labels, predictions)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/metrics.py:231 __call__  **\n        replica_local_fn, *args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/distribute/distributed_training_utils.py:1133 call_replica_local_fn\n        return fn(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/metrics.py:211 replica_local_fn\n        update_op = self.update_state(*args, **kwargs)  # pylint: disable=not-callable\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/metrics_utils.py:90 decorated\n        update_op = update_state_fn(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/metrics.py:176 update_state_fn\n        return ag_update_state(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/metrics.py:612 update_state  **\n        matches = ag_fn(y_true, y_pred, **self._fn_kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/metrics.py:3301 sparse_categorical_accuracy\n        y_true = array_ops.squeeze(y_true, [-1])\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py:507 new_func\n        return func(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py:4259 squeeze\n        return gen_array_ops.squeeze(input, axis, name)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py:10044 squeeze\n        \"Squeeze\", input=input, squeeze_dims=axis, name=name)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:744 _apply_op_helper\n        attrs=attr_protos, op_def=op_def)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py:593 _create_op_internal\n        compute_device)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:3485 _create_op_internal\n        op_def=op_def)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:1975 __init__\n        control_input_ops, op_def)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:1815 _create_c_op\n        raise ValueError(str(e))\n\n    ValueError: Can not squeeze dim[1], expected a dimension of 1, got 10 for '{{node Squeeze}} = Squeeze[T=DT_FLOAT, squeeze_dims=[-1]](labels)' with input shapes: [32,10].\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-467a8a289c50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtest_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_ds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    821\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    824\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    695\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    696\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 697\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2853\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2854\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2855\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2856\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3213\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3215\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3073\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3074\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3075\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3076\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3077\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    971\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    974\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    <ipython-input-47-33adeeb473f7>:8 train_step  *\n        train_accuracy(labels, predictions)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/metrics.py:231 __call__  **\n        replica_local_fn, *args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/distribute/distributed_training_utils.py:1133 call_replica_local_fn\n        return fn(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/metrics.py:211 replica_local_fn\n        update_op = self.update_state(*args, **kwargs)  # pylint: disable=not-callable\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/metrics_utils.py:90 decorated\n        update_op = update_state_fn(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/metrics.py:176 update_state_fn\n        return ag_update_state(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/metrics.py:612 update_state  **\n        matches = ag_fn(y_true, y_pred, **self._fn_kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/metrics.py:3301 sparse_categorical_accuracy\n        y_true = array_ops.squeeze(y_true, [-1])\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py:507 new_func\n        return func(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py:4259 squeeze\n        return gen_array_ops.squeeze(input, axis, name)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py:10044 squeeze\n        \"Squeeze\", input=input, squeeze_dims=axis, name=name)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:744 _apply_op_helper\n        attrs=attr_protos, op_def=op_def)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py:593 _create_op_internal\n        compute_device)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:3485 _create_op_internal\n        op_def=op_def)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:1975 __init__\n        control_input_ops, op_def)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:1815 _create_c_op\n        raise ValueError(str(e))\n\n    ValueError: Can not squeeze dim[1], expected a dimension of 1, got 10 for '{{node Squeeze}} = Squeeze[T=DT_FLOAT, squeeze_dims=[-1]](labels)' with input shapes: [32,10].\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "    \n",
    "    for images, labels in train_ds:\n",
    "        train_step(images, labels)\n",
    "    \n",
    "    for test_images, test_labels in test_ds:\n",
    "        test_step(test_images, test_labels)\n",
    "    \n",
    "    print(\n",
    "    f'Epoch {epoch + 1}, '\n",
    "    f'Loss: {train_loss.result()}, '\n",
    "    f'Accuracy: {train_accuracy.result() * 100}, '\n",
    "    f'Test Loss: {test_loss.result()}, '\n",
    "    f'Test Accuracy: {test_accuracy.result() * 100}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model by learning the optimal weights\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(0.5) # 1.Initialize optimizer with learning rate of 0.5\n",
    "\n",
    "for j in range(1000): # 2. Search for the optimal values for the weights\n",
    "    prediction = nn_forward(flat_layer, weight_fc, bias_fc, weight_out, bias_out)\n",
    "    opt.minimize(lambda: nn_loss_function(flat_layer, weight_fc, bias_fc, weight_out, bias_out, y_image), var_list=[weight_fc, bias_fc, weight_out, bias_out])\n",
    "    correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(y_image, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print out the optimized weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_fc.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_out.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make prediction with the optimal weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = nn_forward(flat_layer, weight_fc, bias_fc, weight_out, bias_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "sum((tf.argmax(prediction, axis=1)==y_image).numpy())/len(y_image.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
