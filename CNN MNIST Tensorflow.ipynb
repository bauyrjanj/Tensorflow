{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of:\n",
      " - Training-set:\t\t60000\n",
      " - Test-set:\t\t\t10000\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of:\")\n",
    "print(\" - Training-set:\\t\\t{}\".format(len(x_train)))\n",
    "print(\" - Test-set:\\t\\t\\t{}\".format(len(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper function for plotting the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(images, cls_true, cls_pred=None):\n",
    "    assert len(images) == len(cls_true) == 9\n",
    "    \n",
    "    # Create figure with 3x3 sub-plots.\n",
    "    fig, axes = plt.subplots(3, 3)\n",
    "    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        # Plot image.\n",
    "        ax.imshow(images[i].reshape(img_shape), cmap='binary')\n",
    "\n",
    "        # Show true and predicted classes.\n",
    "        if cls_pred is None:\n",
    "            xlabel = \"True: {0}\".format(cls_true[i])\n",
    "        else:\n",
    "            xlabel = \"True: {0}, Pred: {1}\".format(cls_true[i], cls_pred[i])\n",
    "\n",
    "        # Show the classes as the label on the x-axis.\n",
    "        ax.set_xlabel(xlabel)\n",
    "        \n",
    "        # Remove ticks from the plot.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    \n",
    "    # Ensure the plot is shown correctly with multiple plots\n",
    "    # in a single Notebook cell.\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We know that MNIST images are 28 pixels by 28 pixels\n",
    "img_size = 28\n",
    "\n",
    "# Images are stored in one-dimensional arrays of this length - number of numbers in the matrix of size 28 x 28\n",
    "img_size_flat = img_size*img_size\n",
    "\n",
    "# Height and width of each image in Tuple\n",
    "img_shape = (img_size, img_size)\n",
    "\n",
    "# Number of color channels for the image: 1 channel for gray-scale\n",
    "num_channels = 1\n",
    "\n",
    "# Number of classes, one class for each of 10 digits\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualise Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the first images from the train set.\n",
    "images = x_train[0:9]\n",
    "\n",
    "# Get the true classes for those images.\n",
    "cls_true = y_train[0:9]\n",
    "\n",
    "# Plot the images and labels using our helper-function above.\n",
    "plot_images(images=images, cls_true=cls_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters of the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional layer 1\n",
    "filter1_size = 5  #Convolution filters are 5 x 5 pixels\n",
    "num_filters1 = 16 #There are 16 of these filters\n",
    "\n",
    "# Convolutional layer 2\n",
    "filter2_size = 5 #Convolution filters are 5 x 5 pixels\n",
    "num_filters2 = 32 #There are 32 of these filters\n",
    "\n",
    "# Pooling\n",
    "window_size = 2 #Pooling window 2x2\n",
    "window_stride = 2 #Move by 2 strides\n",
    "\n",
    "# Fully-connected layer\n",
    "fc_size=128     # Number of nodes in the fully-connected layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform the input data into the required shape/format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 28, 28)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[:100].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the input data\n",
    "x = tf.constant(x_train[:100], tf.float32)\n",
    "y = tf.constant(y_train[:100], tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([100, 28, 28]), TensorShape([100]))"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape as required for ConvNet\n",
    "x_image = tf.reshape(x, [-1, img_size, img_size, num_channels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([100, 28, 28, 1])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper function for Convolutional Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(inputs, filter_size, num_channels, num_filters, img_size, conv_stride, pooling_window_size, pooling_window_stride):\n",
    "    # Define shape for weights\n",
    "    shape_weights = [filter_size, filter_size, num_channels, num_filters]\n",
    "    # Define weights\n",
    "    weights = tf.Variable(tf.random.normal(shape=shape_weights, stddev=0.05))\n",
    "    # Define shape for Bias\n",
    "    shape_bias = [1, img_size, img_size, num_filters]\n",
    "    # Define Bias\n",
    "    bias = tf.Variable(tf.ones(shape=shape_bias))\n",
    "    # Define convolutional layer\n",
    "    conv = tf.nn.conv2d(input=inputs, filters=weights, strides=conv_stride, padding='SAME')\n",
    "    # Add bias\n",
    "    conv+=bias\n",
    "    # Apply RELU activation function\n",
    "    conv = tf.nn.relu(conv)\n",
    "    # Apply Max Pooling\n",
    "    conv = tf.nn.max_pool(input=conv, ksize=pooling_window_size, strides=pooling_window_stride, padding='SAME')\n",
    "    return conv, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate 1st convolutional layer\n",
    "conv1, weights1 = conv_layer(x_image, filter1_size, 1, num_filters1, img_size, 1, window_size, window_stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([100, 14, 14, 16])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([5, 5, 1, 16])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate 2nd convolutional layer\n",
    "conv2, weights2 = conv_layer(conv1, filter2_size, num_filters1, num_filters2, conv1.shape[1], 1, window_size, window_stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([100, 7, 7, 32])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([5, 5, 16, 32])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper-function for flattening a layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_layer(layer):\n",
    "    # Get the shape of the layer\n",
    "    layer_shape = layer.get_shape()\n",
    "    # Calculate the number of features which is: img_height * img_width * num_channels\n",
    "    num_features = layer_shape[1:4].num_elements()\n",
    "    # Reshape the layer to [num_images, num_features].\n",
    "    # Note that we just set the size of the second dimension\n",
    "    # to num_features and the size of the first dimension to -1\n",
    "    # which means the size in that dimension is calculated\n",
    "    # so the total size of the tensor is unchanged from the reshaping.\n",
    "    flat_layer = tf.reshape(layer, [-1, num_features])\n",
    "    return flat_layer, num_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper-function for the fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the architecture of the fully connected network\n",
    "def fc_layer(inputs, num_features, fc_size):\n",
    "    # Define the weights\n",
    "    weights = tf.Variable(tf.random.normal(shape=[num_features, fc_size]))\n",
    "    # Matrix multiply the input and the weights\n",
    "    layer_product = tf.matmul(inputs, weights)\n",
    "    # Initialize the layer bias: one bias for each node\n",
    "    bias = tf.Variable(tf.ones([fc_size]))\n",
    "    # Construct the fully connected layer\n",
    "    layer = tf.nn.relu(layer_product+bias)\n",
    "    return layer, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper-function for the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def out_layer(inputs, num_feature, num_nodes):\n",
    "    # Define weights\n",
    "    weights = tf.Variable(tf.random.normal(shape=[num_feature, num_nodes]))\n",
    "    # Matrix multiply the input and the weights\n",
    "    layer_product = tf.matmul(inputs, weights)\n",
    "    # Initialize the layer bias\n",
    "    bias = tf.Variable(tf.ones([num_nodes]))\n",
    "    # Construct the output layer\n",
    "    layer = tf.nn.softmax(layer_product+bias)\n",
    "    return layer, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the output of 2nd convolutional layer \n",
    "flat_layer, num_features = flatten_layer(conv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([100, 1568])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_layer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1568"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the weights for fully connected layer\n",
    "w1 = tf.Variable(tf.random.normal(shape=[num_features, fc_size]))\n",
    "# Initialize the bias for fully connected layer\n",
    "b1 = tf.Variable(tf.ones([fc_size]))\n",
    "# Define weights for the output layer\n",
    "w_o = tf.Variable(tf.random.normal(shape=[fc_size, num_classes]))\n",
    "# Initialize the layer bias\n",
    "b_o = tf.Variable(tf.ones([num_classes]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the architecture of the network and the model\n",
    "def nn_forward(inputs, w1, b1, w_o, b_o):\n",
    "    fc_product = tf.matmul(inputs, w1)\n",
    "    fully_connected = tf.keras.activations.relu(fc_product+b1)\n",
    "    output_product = tf.matmul(fully_connected, w_o)\n",
    "    output = tf.keras.activations.softmax(output_product+b_o)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper-function for the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_loss_function(inputs, w1, b1, w_o, b_o, actual):\n",
    "    prediction = nn_forward(inputs, w1, b1, w_o, b_o)\n",
    "    y_pred_cls = tf.argmax(prediction, axis=1)\n",
    "    loss = tf.keras.losses.categorical_crossentropy(actual, y_pred_cls)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "cannot compute Mul as input #1(zero-based) was expected to be a int64 tensor but is a double tensor [Op:Mul]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-156-dd4e024d1e61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# 2. Search for the optimal values for the weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnn_loss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_o\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_o\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_o\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_o\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, var_list, grad_loss, name)\u001b[0m\n\u001b[1;32m    373\u001b[0m     \"\"\"\n\u001b[1;32m    374\u001b[0m     grads_and_vars = self._compute_gradients(\n\u001b[0;32m--> 375\u001b[0;31m         loss, var_list=var_list, grad_loss=grad_loss)\n\u001b[0m\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36m_compute_gradients\u001b[0;34m(self, loss, var_list, grad_loss)\u001b[0m\n\u001b[1;32m    427\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m       \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m       \u001b[0mvar_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-156-dd4e024d1e61>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# 2. Search for the optimal values for the weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnn_loss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_o\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_o\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_o\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_o\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-155-3e4729c034e0>\u001b[0m in \u001b[0;36mnn_loss_function\u001b[0;34m(inputs, w1, b1, w_o, b_o, actual)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_o\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_o\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0my_pred_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategorical_crossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py\u001b[0m in \u001b[0;36mcategorical_crossentropy\u001b[0;34m(y_true, y_pred, from_logits, label_smoothing)\u001b[0m\n\u001b[1;32m   1533\u001b[0m   y_true = smart_cond.smart_cond(label_smoothing,\n\u001b[1;32m   1534\u001b[0m                                  _smooth_labels, lambda: y_true)\n\u001b[0;32m-> 1535\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategorical_crossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_logits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrom_logits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mcategorical_crossentropy\u001b[0;34m(target, output, from_logits, axis)\u001b[0m\n\u001b[1;32m   4706\u001b[0m   \u001b[0mepsilon_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_constant_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4707\u001b[0m   \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_by_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mepsilon_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4708\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1125\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m         \u001b[0;31m# Even if dispatching the op failed, the RHS may be a tensor aware\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36m_mul_dispatch\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   1454\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1455\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1456\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mmultiply\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m    506\u001b[0m   \"\"\"\n\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmul\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   6164\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6165\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6166\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6167\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6168\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6841\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6842\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6843\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6844\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: cannot compute Mul as input #1(zero-based) was expected to be a int64 tensor but is a double tensor [Op:Mul]"
     ]
    }
   ],
   "source": [
    "# Train the model by learning the optimal weights\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(0.5) # 1.Initialize optimizer with learning rate of 0.5\n",
    "\n",
    "for j in range(100): # 2. Search for the optimal values for the weights\n",
    "    opt.minimize(lambda: nn_loss_function(flat_layer, w1, b1, w_o, b_o, y), var_list=[w1, b1, w_o, b_o]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
