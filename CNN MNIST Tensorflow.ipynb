{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of:\n",
      " - Training-set:\t\t60000\n",
      " - Test-set:\t\t\t10000\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of:\")\n",
    "print(\" - Training-set:\\t\\t{}\".format(len(x_train)))\n",
    "print(\" - Test-set:\\t\\t\\t{}\".format(len(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (60000,), (10000, 28, 28), (10000,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper function for plotting the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(images, cls_true, cls_pred=None):\n",
    "    assert len(images) == len(cls_true) == 9\n",
    "    \n",
    "    # Create figure with 3x3 sub-plots.\n",
    "    fig, axes = plt.subplots(3, 3)\n",
    "    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        # Plot image.\n",
    "        ax.imshow(images[i].reshape(img_shape), cmap='binary')\n",
    "\n",
    "        # Show true and predicted classes.\n",
    "        if cls_pred is None:\n",
    "            xlabel = \"True: {0}\".format(cls_true[i])\n",
    "        else:\n",
    "            xlabel = \"True: {0}, Pred: {1}\".format(cls_true[i], cls_pred[i])\n",
    "\n",
    "        # Show the classes as the label on the x-axis.\n",
    "        ax.set_xlabel(xlabel)\n",
    "        \n",
    "        # Remove ticks from the plot.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    \n",
    "    # Ensure the plot is shown correctly with multiple plots\n",
    "    # in a single Notebook cell.\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We know that MNIST images are 28 pixels by 28 pixels\n",
    "img_size = 28\n",
    "\n",
    "# Images are stored in one-dimensional arrays of this length - number of numbers in the matrix of size 28 x 28\n",
    "img_size_flat = img_size*img_size\n",
    "\n",
    "# Height and width of each image in Tuple\n",
    "img_shape = (img_size, img_size)\n",
    "\n",
    "# Number of color channels for the image: 1 channel for gray-scale\n",
    "num_channels = 1\n",
    "\n",
    "# Number of classes, one class for each of 10 digits\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualise Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the first images from the train set.\n",
    "images = x_train[0:9]\n",
    "\n",
    "# Get the true classes for those images.\n",
    "cls_true = y_train[0:9]\n",
    "\n",
    "# Plot the images and labels using our helper-function above.\n",
    "plot_images(images=images, cls_true=cls_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters of the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional layer 1\n",
    "filter1_size = 5  #Convolution filters are 5 x 5 pixels\n",
    "num_filters1 = 16 #There are 16 of these filters\n",
    "\n",
    "# Convolutional layer 2\n",
    "filter2_size = 5 #Convolution filters are 5 x 5 pixels\n",
    "num_filters2 = 32 #There are 32 of these filters\n",
    "\n",
    "# Pooling\n",
    "window_size = 2 #Pooling window 2x2\n",
    "window_stride = 2 #Move by 2 strides\n",
    "\n",
    "# Fully-connected layer\n",
    "fc_size=128     # Number of nodes in the fully-connected layer\n",
    "\n",
    "# Convolution stride\n",
    "conv_stride=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform the input data into the required shape/format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 28, 28)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[:100].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the input data\n",
    "x = tf.constant(x_train[:100], tf.float32)\n",
    "y = tf.constant(y_train[:100], tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([100, 28, 28]), TensorShape([100]))"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape as required for ConvNet\n",
    "x_image = tf.reshape(x, [-1, img_size, img_size, num_channels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([100, 28, 28, 1])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Parameters for Convolutional Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define shape for conv1 weights\n",
    "conv1_weight_shape = [filter1_size, filter1_size, num_channels, num_filters1]\n",
    "# Define conv1 weights\n",
    "conv1_weights = tf.Variable(tf.random.normal(shape=conv1_weight_shape, stddev=0.05))\n",
    "\n",
    "# Define shape for conv2 weights\n",
    "conv2_weight_shape = [filter2_size, filter2_size, num_filters1, num_filters2]\n",
    "# Define conv2 weights\n",
    "conv2_weights = tf.Variable(tf.random.normal(shape=conv2_weight_shape, stddev=0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([5, 5, 1, 16])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = tf.nn.conv2d(input=x_image, filters=conv1_weights, strides=conv_stride, padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([100, 28, 28, 16])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper function for Convolutional Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(inputs, weights, num_filters, img_size, conv_stride, pooling_window_size, pooling_window_stride):\n",
    "    # Define shape for bias\n",
    "    bias_shape = [1, img_size, img_size, num_filters]\n",
    "    # Define bias\n",
    "    bias = tf.Variable(tf.ones(shape=bias_shape))\n",
    "    # Define convolutional layer\n",
    "    conv = tf.nn.conv2d(input=inputs, filters=weights, strides=conv_stride, padding='SAME')\n",
    "    # Add bias\n",
    "    conv+=bias\n",
    "    # Apply RELU activation function\n",
    "    conv = tf.nn.relu(conv)\n",
    "    # Apply Max Pooling\n",
    "    conv = tf.nn.max_pool(input=conv, ksize=pooling_window_size, strides=pooling_window_stride, padding='SAME')\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate 1st convolutional layer\n",
    "conv1 = conv_layer(x_image, conv1_weights, num_filters1, img_size, conv_stride, window_size, window_stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([100, 14, 14, 16])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate 2nd convolutional layer\n",
    "conv2 = conv_layer(conv1, conv2_weights, num_filters2, conv1.shape[1], conv_stride, window_size, window_stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([100, 7, 7, 32])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper-function for flattening a layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_layer(layer):\n",
    "    # Get the shape of the layer\n",
    "    layer_shape = layer.get_shape()\n",
    "    # Calculate the number of features which is: img_height * img_width * num_channels\n",
    "    num_features = layer_shape[1:4].num_elements()\n",
    "    # Reshape the layer to [num_images, num_features].\n",
    "    # Note that we just set the size of the second dimension\n",
    "    # to num_features and the size of the first dimension to -1\n",
    "    # which means the size in that dimension is calculated\n",
    "    # so the total size of the tensor is unchanged from the reshaping.\n",
    "    flat_layer = tf.reshape(layer, [-1, num_features])\n",
    "    return flat_layer, num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the output of 2nd convolutional layer \n",
    "flat_layer, num_features = flatten_layer(conv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([100, 1568])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_layer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1568"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Parameters for the Fully Connected and Output Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the weights for fully connected layer\n",
    "weight_fc = tf.Variable(tf.random.normal(shape=[num_features, fc_size]))\n",
    "# Initialize the bias for fully connected layer\n",
    "bias_fc = tf.Variable(tf.ones([fc_size]))\n",
    "\n",
    "# Define weights for the output layer\n",
    "weight_out = tf.Variable(tf.random.normal(shape=[fc_size, num_classes]))\n",
    "# Initialize the layer bias\n",
    "bias_out = tf.Variable(tf.ones([num_classes]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct the Forward Feeding Fully Connected Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the architecture of the model\n",
    "def nn_forward(features, fc_weights, fc_bias, w_out, b_out):\n",
    "    # Fully connected layer\n",
    "    fc_product = tf.matmul(features, fc_weights)\n",
    "    # Apply activation function to the fully connected layer\n",
    "    fully_connected = tf.keras.activations.relu(fc_product+fc_bias)\n",
    "    \n",
    "    # Output layer\n",
    "    output = tf.matmul(fully_connected, w_out)\n",
    "    prediction = tf.keras.activations.softmax(output + b_out)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper-function for the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_loss_function(features, fc_weights, fc_bias, w_out, b_out, actual):\n",
    "    prediction = nn_forward(features, fc_weights, fc_bias, w_out, b_out)\n",
    "    # y_pred_cls = tf.argmax(prediction, axis=1)\n",
    "    loss = tf.keras.losses.categorical_crossentropy(tf.one_hot(actual,depth=10), prediction)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model by learning the optimal weights\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(0.5) # 1.Initialize optimizer with learning rate of 0.5\n",
    "\n",
    "for j in range(1000): # 2. Search for the optimal values for the weights\n",
    "    opt.minimize(lambda: nn_loss_function(flat_layer, weight_fc, bias_fc, weight_out, bias_out, y), var_list=[weight_fc, bias_fc, weight_out, bias_out]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print out the optimized weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.7494996 ,  0.90472984, -1.4861904 , ..., -2.0257025 ,\n",
       "        -0.26843092,  0.74748397],\n",
       "       [ 3.2670813 , -0.6290051 ,  0.70887274, ...,  2.2285535 ,\n",
       "         0.919856  , -0.60224897],\n",
       "       [-0.4061744 ,  1.1644459 ,  0.42442897, ..., -0.91974425,\n",
       "        -0.32629436, -0.04603577],\n",
       "       ...,\n",
       "       [ 0.74483013,  0.60290223,  0.16054599, ...,  1.2177017 ,\n",
       "         0.18758899,  0.8942129 ],\n",
       "       [ 0.24023798, -0.92563164, -0.4458919 , ...,  0.71106815,\n",
       "         0.53823924, -0.0220941 ],\n",
       "       [ 1.2525358 , -0.23145513, -1.410284  , ...,  1.9206133 ,\n",
       "         0.5851155 , -0.46860445]], dtype=float32)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_fc.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.30805564,  0.5633705 ,  0.60768265, ...,  1.2214241 ,\n",
       "         0.4821326 , -0.27678102],\n",
       "       [ 0.29177448, -0.46370003, -0.78041875, ...,  1.7461033 ,\n",
       "        -0.10775954,  0.45852384],\n",
       "       [-0.60127527,  0.9484125 , -0.67825943, ...,  0.0676578 ,\n",
       "        -0.73224324,  1.5672511 ],\n",
       "       ...,\n",
       "       [-0.8570307 , -2.4102602 ,  0.11767936, ..., -0.29490712,\n",
       "        -3.581652  , -0.5767586 ],\n",
       "       [-0.79403406, -0.01325877,  1.7398436 , ..., -0.81428653,\n",
       "         0.44500184,  0.6659394 ],\n",
       "       [ 1.4044378 , -1.1636358 ,  0.81251997, ..., -0.49612087,\n",
       "         0.42762795, -1.4269898 ]], dtype=float32)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_out.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make prediction with the optimal weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = nn_forward(flat_layer, weight_fc, bias_fc, weight_out, bias_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy\n",
    "sum((tf.argmax(prediction, axis=1)==y).numpy())/len(y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
